import os
import dgl
import random
import torch as th
import pandas as pd
import numpy as np
import math
from scipy.spatial.distance import euclidean

random.seed(24)


def load_to_dgl_graph(dataset, s):  # å¤„ç†å›¾æ•°æ®å¹¶è¿›è¡Œå¼‚å¸¸æ³¨å…¥å®žéªŒ
    edges = pd.read_csv(os.path.join('../Data/', dataset, '{}.txt'.format(dataset)), sep=' ',
                        names=['start_idx', 'end_idx', 'time'])  # è¾¹

    src_nid = edges.start_idx.to_numpy()  # æºèŠ‚ç‚¹
    dst_nid = edges.end_idx.to_numpy()  # ç›®æ ‡èŠ‚ç‚¹

    graph = dgl.graph((src_nid, dst_nid))  # æž„å»ºå›¾
    graph.edata['time'] = th.Tensor(edges.time.tolist())  # å­˜å‚¨è¾¹ä¸­çš„æ—¶é—´ä¿¡æ¯

    node_feat = position_encoding(max_len=graph.num_nodes(), emb_size=128)  # èŠ‚ç‚¹ä½ç½®ç¼–ç 

    # m: num of fully connected nodes
    # n: num of fully connected clusters
    # k: another ð‘˜ nodes as a candidate set
    m, n, k = 15, 20, 50

    if dataset == 'bitcoinotc' or dataset == 'bitotc' or dataset == 'bitalpha':
        n = 10
    elif dataset == 'dblp' or dataset == 'tax':
        n = 20
    elif dataset == 'tax51' or dataset == 'reddit':
        n = 200
    inject_graph, inject_node_feat, anomaly_label = inject_anomaly(graph, node_feat, m, n, k, s)  # å¼‚å¸¸æ³¨å…¥

    return inject_graph, inject_node_feat, anomaly_label


def inject_anomaly(g, feat, m, n, k, s):  # å¼‚å¸¸æ³¨å…¥
    num_node = g.num_nodes()  # å›¾èŠ‚ç‚¹æ•°
    all_idx = list(range(g.num_nodes()))  # ç´¢å¼•åˆ—è¡¨
    random.shuffle(all_idx)  # æ‰“ä¹±ç´¢å¼•åˆ—è¡¨
    anomaly_idx = all_idx[:m * n * 2]  # å‰ m * n * 2 ä¸ªä¸ºå¼‚å¸¸ç´¢å¼•

    structure_anomaly_idx = anomaly_idx[:m * n]  # ç»“æž„å¼‚å¸¸
    attribute_anomaly_idx = anomaly_idx[m * n:]  # å±žæ€§å¼‚å¸¸
    label = np.zeros((num_node, 1), dtype=np.uint8)
    label[anomaly_idx, 0] = 1  # å¼‚å¸¸èŠ‚ç‚¹å…¨ 1, å…¶ä½™èŠ‚ç‚¹å…¨ 0

    str_anomaly_label = np.zeros((num_node, 1), dtype=np.uint8)  # æ ‡è®°ç»“æž„å¼‚å¸¸èŠ‚ç‚¹
    str_anomaly_label[structure_anomaly_idx, 0] = 1
    attr_anomaly_label = np.zeros((num_node, 1), dtype=np.uint8)  # æ ‡è®°å±žæ€§å¼‚å¸¸èŠ‚ç‚¹
    attr_anomaly_label[attribute_anomaly_idx, 0] = 1

    # Disturb structure
    print('Constructing structured anomaly nodes...')  # æž„é€ ç»“æž„å¼‚å¸¸èŠ‚ç‚¹
    u_list, v_list, t_list = [], [], []  # å­˜å‚¨æ–°è¾¹çš„èµ·å§‹èŠ‚ç‚¹ã€ç»ˆæ­¢èŠ‚ç‚¹å’Œæ—¶é—´æˆ³
    max_time, min_time = max(g.edata['time'].tolist()), min(g.edata['time'].tolist())
    for n_ in range(n):  # åœ¨æ¯ä¸ªå®Œå…¨è¿žæŽ¥çš„ç°‡ä¸­ï¼Œä¸ºèŠ‚ç‚¹å¯¹æ·»åŠ è¾¹
        current_nodes = structure_anomaly_idx[n_ * m:(n_ + 1) * m]
        t = random.uniform(min_time, max_time)
        for i in current_nodes:
            for j in current_nodes:
                u_list.append(i)
                v_list.append(j)
                t_list.append(t)

    ori_num_edge = g.num_edges()  # åŽŸå§‹è¾¹æ•°
    g = dgl.add_edges(g, th.tensor(u_list), th.tensor(v_list), {'time': th.tensor(t_list)})  # æ·»åŠ æ–°è¾¹

    num_add_edge = g.num_edges() - ori_num_edge  # æ·»åŠ çš„è¾¹æ•°
    print('Done. {:d} structured nodes are constructed. ({:.0f} edges are added) \n'.format(len(structure_anomaly_idx),
                                                                                            num_add_edge))

    # Disturb attribute
    print('Constructing attributed anomaly nodes...')  # æž„é€ å±žæ€§å¼‚å¸¸èŠ‚ç‚¹
    feat_list = []
    ori_feat = feat
    attribute_anomaly_idx_list = split_list(attribute_anomaly_idx, s)  # æ‰“ä¹±å±žæ€§å¼‚å¸¸èŠ‚ç‚¹çš„ç‰¹å¾, å°†å±žæ€§å¼‚å¸¸èŠ‚ç‚¹ç´¢å¼•æ‹†åˆ†æˆ s ä¸ªå­é›†
    for lst in attribute_anomaly_idx_list:  # å¯¹æ¯ä¸ªå±žæ€§å¼‚å¸¸èŠ‚ç‚¹å­é›†, é€‰æ‹© k ä¸ªå€™é€‰èŠ‚ç‚¹, è®¡ç®—å½“å‰èŠ‚ç‚¹ç‰¹å¾ä¸Žå€™é€‰èŠ‚ç‚¹ç‰¹å¾ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦», å°†æœ€å¤§è·ç¦»çš„å€™é€‰èŠ‚ç‚¹ç‰¹å¾èµ‹ç»™å½“å‰èŠ‚ç‚¹
        feat = ori_feat
        for i_ in lst:
            picked_list = random.sample(all_idx, k)
            max_dist = 0
            for j_ in picked_list:
                cur_dist = euclidean(ori_feat[i_], ori_feat[j_])
                if cur_dist > max_dist:
                    max_dist = cur_dist
                    max_idx = j_
            feat[i_] = feat[max_idx]
        feat_list.append(feat)
    print('Done. {:d} attributed nodes are constructed. \n'.format(len(attribute_anomaly_idx)))

    return g, feat_list, label


def dataloader(dataset):  # åŠ è½½æ•°æ®
    edges = pd.read_csv(os.path.join('../Data/', dataset, '{}.txt'.format(dataset)), sep=' ',
                        names=['start_idx', 'end_idx', 'time'])  # è¾¹
    label = pd.read_csv(os.path.join('../Data/', dataset, 'node2label.txt'), sep=' ', names=['nodeidx', 'label'])  # æ ‡ç­¾

    src_nid = edges.start_idx.to_numpy()  # æºèŠ‚ç‚¹
    dst_nid = edges.end_idx.to_numpy()  # ç›®æ ‡èŠ‚ç‚¹

    graph = dgl.graph((src_nid, dst_nid))  # æž„å»ºå›¾

    labels = th.full((graph.number_of_nodes(),), -1).cuda()  # å­˜å‚¨æ ‡ç­¾

    nodeidx, lab = label.nodeidx.tolist(), label.label.tolist()  # ç´¢å¼•åˆ—è¡¨, æ ‡ç­¾åˆ—è¡¨

    for i in range(len(nodeidx)):
        labels[nodeidx[i]] = lab[i] - min(lab)

    train_mask = th.full((graph.number_of_nodes(),), False)  # è®­ç»ƒé›†æŽ©ç 
    val_mask = th.full((graph.number_of_nodes(),), False)  # éªŒè¯é›†æŽ©ç 
    test_mask = th.full((graph.number_of_nodes(),), False)  # æµ‹è¯•é›†æŽ©ç 

    random.seed(24)
    train_mask_index, val_mask_index, test_mask_index = th.LongTensor([]), th.LongTensor([]), th.LongTensor([])
    for i in range(min(labels), max(labels) + 1):  # åˆ’åˆ†æ•°æ®é›†
        index = [j for j in label[label.label == i].nodeidx.tolist()]
        random.shuffle(index)
        train_mask_index = th.cat((train_mask_index, th.LongTensor(index[:int(len(index) / 10)])), 0)
        val_mask_index = th.cat((val_mask_index, th.LongTensor(index[int(len(index) / 10):int(len(index) / 5)])), 0)
        test_mask_index = th.cat((test_mask_index, th.LongTensor(index[int(len(index) / 5):])), 0)

    train_mask.index_fill_(0, train_mask_index, True).cuda()
    val_mask.index_fill_(0, val_mask_index, True).cuda()
    test_mask.index_fill_(0, test_mask_index, True).cuda()
    train_idx = th.nonzero(train_mask, as_tuple=False).squeeze()  # è®­ç»ƒé›†ç´¢å¼•
    val_idx = th.nonzero(val_mask, as_tuple=False).squeeze()  # éªŒè¯é›†ç´¢å¼•
    test_idx = th.nonzero(test_mask, as_tuple=False).squeeze()  # æµ‹è¯•é›†ç´¢å¼•
    n_classes = label.label.nunique()  # ç±»åˆ«æ•°

    return labels, train_idx, val_idx, test_idx, n_classes


def position_encoding(max_len, emb_size):  # ä½ç½®ç¼–ç 
    pe = th.zeros(max_len, emb_size)  # å­˜å‚¨ä½ç½®ç¼–ç 
    position = th.arange(0, max_len).unsqueeze(1)  # ä½ç½®ç´¢å¼•

    div_term = th.exp(th.arange(0, emb_size, 2) * -(math.log(10000.0) / emb_size))  # å­˜æœ‰ä½ç½®ç¼–ç çš„ç¼©æ”¾å› å­

    pe[:, 0::2] = th.sin(position * div_term)  # å¯¹å¶æ•°åˆ—èµ‹å€¼
    pe[:, 1::2] = th.cos(position * div_term)  # å¯¹å¥‡æ•°åˆ—èµ‹å€¼
    return pe


def split_list(lst, s):
    avg_length = len(lst) // s
    remainder = len(lst) % s
    result = [lst[i * avg_length + min(i, remainder):(i + 1) * avg_length + min(i + 1, remainder)] for i in range(s)]
    return result


def sampling_layer(snapshots, views, span, strategy):  # é‡‡æ ·
    T = []
    if strategy == 'random':  # éšæœºç­–ç•¥
        T = [random.uniform(0, span * (snapshots - 1) / snapshots) for _ in range(views)]
    elif strategy == 'low_overlap':  # ä½Žé‡å ç­–ç•¥
        if (0.75 * views + 0.25) > snapshots:
            return "The number of sampled views exceeds the maximum value of the current policy."
        start = random.uniform(0, span - (0.75 * views + 0.25) * span / snapshots)
        T = [start + (0.75 * i * span) / snapshots for i in range(views)]
    elif strategy == 'high_overlap':  # é«˜é‡å ç­–ç•¥
        if (0.25 * views + 0.75) > snapshots:
            return "The number of sampled views exceeds the maximum value of the current policy."
        start = random.uniform(0, span - (0.25 * views + 0.75) * span / snapshots)
        T = [start + (0.25 * i * span) / snapshots for i in range(views)]
    elif strategy == 'sequential':  # é¡ºåºç­–ç•¥
        T = [span * i / snapshots for i in range(snapshots)]
        ori_T = T
        if views > snapshots:
            return "The number of sampled views exceeds the maximum value of the current policy."
        T = random.sample(T, views)
        T_idx = [ori_T.index(i) for i in T]

    return T, T_idx
